---
title: "Denoise & Decontaminate ASVs"
author: "M Fisher via Ramon Gallego"
date: '2022-09-28'
output: html_document
---
After running the demultiplexer_for_dada2 (http://github.com/ramongallego/demultiplexer_for_dada2), we have to denoise the whole dataset. We will do this by using 4 different processes:


  * **Estimation of *Tag-jumping* or indices *cross-talk* **. We run multiple samples on each MiSeq run. These are identified by two sets of molecular barcodes. There is the potential of some sequences to be assigned to the wrong sample, which is a bummer. To estimate how many reads did this, on each MiSeq run we added some samples whose composition is known and extremely unlikely to be present in the enviromental samples studied. AS a result of this **Tag-jumping**, some of the positive control sequences might show in the environmental samples and viceversa. In our case, these positive controls are made of either Kangaroo or Ostrich (and Alligator). The process consists on, for each run, to model the compositon observed on the positive controls and substract it from the environmental samples from that run. The output will be a dataset with the same number of samples as before, but with fewer reads of certain sequences (ASVs)
  
  * **Discarding samples with extremely low number of reads**. Sometimes the number of reads sequenced from a particular replicate are really low, and hence the relative proportions of ASVs would be skewed. 
  
  * **Full clearance from Positive control influence**. THis process also takes advantage of the known composition of the positive controls. Each ASV found in the positive controls with a higher abundace in them than in the rest of the samples will be labelled as  **Positive** and removed from the environmental dataset. The output will be a dataset with the same number of samples as before but with fewer ASVs.
  
  * **Occupancy modelling** . The Kelly Lab doesn't advise occupancy modelling at this step anymore.
  
  * **Dissimilarity between PCR replicates**. The workflow that leads to the sequencing of a particular sample is subject to many stochatic processes, and it is not unlikely that the composition retrieved is very different for the original community. A way to ensure that this difference is minimal is through the separate analysis of each PCR replicate. We used that approach and modeled the dissimilarity between each PCr replicate and the group centroid. This way of modeling the dissimilarity allows us to discard those PCR replicate that won't fit the normal distribution of dissimilarities. The output of this procedure will be a dataset with the same number of **Hashes** as before but with fewer **samples**.
  
  
As with everything, we will start the process by loading the required packages and datasets.

# Load the dataset and metadata



```{r load libraries, include=FALSE}
 knitr::opts_chunk$set()
 library (here)
 library (tidyverse)
 library (vegan)
 #library (MASS)
 library (proxy)
 library (reshape2)
 library (seqinr)
 library (patchwork)
 library (magrittr)

```


Custom functions
```{r}
source(here('R','dist_to_centroid.R'))
source(here('R','how_many.R'))
source(here('R','tibble_to_matrix.R'))
```


User directories
```{r set up }
# directory with ASV table from dada2
dada2_dir <- "data/dada2"
# output directory
outdir    <- "data/dada2"
```

User inputs
```{r}
# metadata file (with directory)
metadat_file <- "data/DCRB_run1_samples.csv"
```
<br>

We will fit the number of reads assigned to each sample to a normal distribution and discard those samples with a probability of ___% of not fitting in that distribution. 
```{r}
pfit <- 0.95
```
<br>
<br>


# Load data

ASV table and starting hash key
```{r}
all.asvs <- read.csv(here(dada2_dir, "ASV_table.csv")) %>% mutate(MiSeqRun=1)
all.hashes <- read.csv(here(dada2_dir, "Hash_key.csv")) %>% mutate(MiSeqRun=1)
```
<br>

metadata
```{r}
all.metadata <- read.csv(here(metadat_file))
```
<br>

filter metadata to only include samples which came out of dada2
```{r}
metadata <- all.metadata %>%
  rename(sample_id=Sample_label,Tag=Sample_name) %>% mutate(MiSeqRun=1,pri_index_name="Lib_A") %>%
  mutate(Tag = paste0("Tag_",Tag)) %>%
  dplyr::select(sample_id, pri_index_name, Tag, MiSeqRun)
```
<br>

## Data Cleanup

A few things we check for: That **no sample appears twice** in the metadata. That the metadata **uses Tag_01 instead of Tag_1** (so it can be sorted alphabetically). That **the structure** Site_YYYYMM[A-C].[1-3] **is the same** across the dataset.

```{r data cleaning}

# Check that no sample appears more than once in the metadata

metadata %>% 
  group_by(sample_id) %>%
  summarise(tot = n()) %>% 
  arrange(desc(tot)) # Samples only appear once

# We should change Tag_1 for Tag_01

metadata <- metadata %>%
  mutate(Tag = ifelse(str_detect(Tag, "\\_[0-9]{1}$"), str_replace(Tag, "Tag_", "Tag_0"),Tag))

```
<br>

The outputs of this process are a clean ASV table and a clean metadata file.


# Decontaminate Data

## Cleaning Process 1: Estimation of *Tag-jumping* or sample *cross-talk*

Before we modify our datasets on any way, we can calculate how many sequences that were only supposed to be in the positives control appeared in the environmental samples, and how many did the opposite. First we divide the dataset into positive control and environmental samples. Also create an ordered list of the Hashes present in the positive controls, for ease of plotting

```{r split into two}
ASV.table <- filter(all.asvs, MiSeqRun==1)
ASV.table %<>%  mutate(source = ifelse(str_detect(sample_id, "Control|Positive\\+|Negative\\+"),"Positives","Samples"))

ASV.table %>% 
  filter (source == "Positives") %>% 
  group_by(Hash) %>% 
  summarise(tot = sum(nReads)) %>% 
  arrange(desc(tot)) %>% 
  pull(Hash) -> good.order


```
<br>
Now let's create a jumping vector. What proportion of the reads found in the positives control come from elsewhere, and what proportion of the reads in the samples come from the positives control.

### Step 1: Nest the dataset and split it in positives and samples

To streamline the process and make it easier to execute it similarly but independently on each Miseq run, we nest the dataset by run. 

So Step1 is create a nested table so we can run this analysis on each run independently. The nested table has three columns: MiSeqRun, Samples, Positives. The "Samples" column contains the ASV table for all samples in the given MiSeqRun. The "Positives" column contains the ASV table for all controls in the given MiSeqRun.

```{r nesting the dataset}
ASV.table %>% 
  group_by(MiSeqRun, source) %>% 
  nest() %>% 
  pivot_wider(names_from = source, values_from =  data) -> ASV.nested 
```
<br>


That wasn't too complicated. Let's use the custom function `how.many` to keep track of our cleaning process. This summary function just counts the number of distinct samples, hashes, and reads from an ASV table. The code applying the function maps it to each dataframe in the "Samples" column of the nested table. 
```{r summary.file}
ASV.nested %>% 
  transmute(Summary = map(Samples, ~ how.many(ASVtable = .,round = 0)))  -> ASV.summary

```
<br>


### Step 2: Model the composition of the positive controls of each run 


We create a vector of the composition of each positive control and substract it from the environmental samples from their runs
```{r jumping vector}
ASV.nested %>% 
  mutate (contam.tibble = map(Positives, 
                              function(.x){
                                .x %>%
                                  group_by(Sample_name,sample_id) %>%    ## changed from group_by(sample) | MCF 10/7
                                  mutate (TotalReadsperSample = sum(nReads)) %>%
                                  mutate (proportion = nReads/TotalReadsperSample) %>%
                                  group_by (Hash) %>%
                                  summarise (vector_contamination = max (proportion))
                                }) ) -> ASV.nested
# Check how it looks across all runs
ASV.nested %>% 
  group_by(MiSeqRun) %>%      ## changed from group_by(Miseq_Run) | MCF 10/7
  select(contam.tibble) %>% 
  unnest(cols = contam.tibble) 
```
<br>

### Step 3: Substract the composition of the positive controls from the environment samples

The idea behind this procedure is that we know, for each run, how many reads from each Hash appeared in the positive controls. These come from 2 processes: sequences we know should appear in the positive controls, and sequences that have *jumped* from the environment to the positive controls. With this procedure, we subtract from every environmental sample the proportion of reads that jumped from elsewhere.

```{r cleaning step 1}
ASV.nested %>% 
  mutate(cleaned.tibble = map2(Samples, contam.tibble, function(.x,.y){ 
    .x %>%
      group_by (Sample_name, sample_id) %>%                       ## changed from group_by(sample) | MCF 10/7
      mutate (TotalReadsperSample = sum (nReads)) %>%
      left_join(.y, by = "Hash") %>%
      mutate (Updated_nReads = ifelse (!is.na(vector_contamination),  nReads - (ceiling(vector_contamination*TotalReadsperSample)), nReads)) %>%
      filter (Updated_nReads > 0) %>%
      ungroup() %>% 
      dplyr::select (Sample_name, sample_id, Hash, nReads = Updated_nReads)
      
    
  })) -> ASV.nested
```
<br>


Add this step to the summary table we were creating to track sample / hash / read counts.
```{r summary.file.2}
ASV.nested %>% 
  transmute( Summary.1 = map(cleaned.tibble, ~ how.many(ASVtable = .,round = "1.Jump"))) %>% 
  left_join(ASV.summary) %>% 
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary 
```
<br>
<br>

## Cleaning Process 2: **Discarding PCR replicates with low number of reads**

We will fit the number of reads assigned to each sample to a normal distribution and discard those samples with a probability of `r pfit`% of not fitting in that distribution. 

First, use the user-specified `pfit` to set a cutoff: 
```{r}
cutoff_prob <- (1-pfit)/2
```
<br>

The output of this filtering would be a dataset with less samples and potentially less number of unique Hashes.
```{r fitting nReads per sample}

ASV.nested %>% 
  select(MiSeqRun,cleaned.tibble) %>% 
  unnest(cleaned.tibble) %>% 
  group_by(Sample_name, sample_id) %>%
  summarise(tot = sum(nReads)) -> all.reps

# Visualize

all.reps %>%  
  pull(tot) -> reads.per.sample

names(reads.per.sample) <- all.reps %>% pull(Sample_name)  

normparams.reads <- MASS::fitdistr(reads.per.sample, "normal")$estimate



all.reps %>%  
  mutate(prob = pnorm(tot, normparams.reads[1], normparams.reads[2])) -> all.reps

#  probs <- pnorm(all_pairwise_distances, normparams[1], normparams[2])

outliers <- 
  all.reps %>% 
  filter(prob < cutoff_prob & tot < normparams.reads[1])

ASV.nested %>% 
  mutate(Step.1.low.reads = map (cleaned.tibble, ~ filter(.,!Sample_name %in% outliers$Sample_name) %>% ungroup)) -> ASV.nested

ASV.nested %>% 
  transmute( Summary.1 = map(Step.1.low.reads, ~ how.many(ASVtable = .,round = "2.Low.nReads"))) %>% 
  left_join(ASV.summary) %>% 
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary 

```
<br>
<br>


## Cleaning Process 3: **Full clearance from Positive control influence**

Removing the Hashes that belong to the positive controls. First, for each Hash that appeared in the positive controls, determine whether a sequence is a true positive or a true environment. For each Hash, we will calculate, maximum, mean and total number of reads in both positive and samples, and then we will use the following decision tree:

  * If all three statistics are higher in one of the groups, we will label it either of Environmental or Positive control influence.
  
  * If there are conflicting results, we will use the Hashes to see if they belong to either group. if the maximum abundance of a Hash is in a positive, then it is a positive, otherwise is a real sequence from the environment.


Now, for each Hash in each set of positives controls, calculate the proportion of reads that were mis-assigned - they appeared somewhere in the samples they were not expected.
We will divide that process in two: first . A second step would be to create a column named proportion switched, which states the proportion of reads from one Hash that jumped from the environment to a positive control or viceversa. The idea is that any presence below a threshold can be arguably belong to tag jumping.

```{r real or positive}
ASV.table %>% 
  filter (Hash %in% good.order) %>%
  group_by(Sample_name, sample_id) %>% 
  mutate(tot.reads = sum(nReads)) %>% 
  group_by(Hash,Sample_name) %>% 
  mutate(prop = nReads/tot.reads) %>% 
  group_by(Hash, source) %>% 
  summarise (max.  = max(prop),
             mean. = mean(prop),
             tot.  = sum(nReads)) %>% 
  gather(contains("."), value = "number", key = "Stat") %>%
  spread(key = "source", value = "number", fill = 0) %>% 
  group_by(Hash, Stat) %>%
  mutate(origin = case_when(Positives > Samples ~ "Positive.control",
                            TRUE                ~ "Environment")) %>% 
  group_by (Hash) %>%
  mutate(tot = n_distinct(origin)) -> Hash.fate.step2

Hash.fate.step2 %>% 
  filter(tot == 1) %>% 
  group_by(Hash) %>% 
  summarise(origin = unique(origin)) %>% 
  filter(origin == "Positive.control") -> Hashes.to.remove.step2

ASV.table %>% 
  group_by(source, Hash) %>% 
  summarise(ocurrences =n()) %>% 
  spread(key = source, value = ocurrences, fill = 0) %>% 
  #left_join(Hashes.to.remove.step2) %>% 
  #mutate(origin = case_when(is.na(origin) ~ "Kept",
   #                         TRUE          ~ "Discarded")) %>% 
  mutate(second.origin = case_when(Positives >= Samples ~ "Discarded",
                                   TRUE                 ~ "Kept")) %>% 
  filter(second.origin == "Discarded") %>% 
  full_join(Hashes.to.remove.step2) -> Hashes.to.remove.step2
```
<br>

There were 4 hashes marked for removal:
```
5ffae7e8304f8eec6ba8d087502e140904c0147
2e46f9be10f6c3fe32646ff7213f9a80843c7e41
3d2237a26590dba4ca8ff8b0470b0b470dc746e9
ff5e108c96a673075a040151a2e3fc99e065a752
```


IN order to train DADA2 to better distinguish when positive control sequences have arrived in the environment, we will keep the sequences in a csv file


```{r ASVs from positives}

Hashes.to.remove.step2 %>% 
  left_join(all.hashes) %>% 
  select(Hash, Sequence) %>% 
  write_csv(here(outdir,"Hashes.to.remove.csv"))

```
<br>

### Remove the positive control hashes from the composition of the ASVs

```{r cleaning.Step2}

ASV.nested %>% 
  mutate(Step2.tibble = map (Step.1.low.reads, ~ filter(.,!Hash %in% Hashes.to.remove.step2$Hash) %>% ungroup)) -> ASV.nested

saveRDS(ASV.nested, file = here(outdir, "Cleaning.before.Occ.model"))

ASV.nested <- readRDS(file =here(outdir, "Cleaning.before.Occ.model"))

ASV.nested %>% 
  transmute( Summary.1 = map(Step2.tibble, ~ how.many(ASVtable = .,round = "3.Positives"))) %>% 
  left_join(ASV.summary) %>% 
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary 
```
<br>

Check on how many samples / hashes were filtered out:
```{r}
View(ASV.summary %>% 
  unnest(cols = c(Summary)))
```
<br>

No hashes were removed from the samples -- I guess that means that the hashes in the controls were only in the controls. 

<br>
<br>

## Cleaning Process 5: **Dissimilarity between PCR replicates**

So, a second way of cleaning the dataset is to remove samples for which the dissimilarity between PCR replicates exceeds the normal distribution of dissimilarities.
Sometimes the preparation of a PCR replicate goes wrong for a number of reasons - that leads to a particular PCR replicate to be substantially different to the other 2. In that case, we will remove the PCR replicate that has higher dissimilarity with the other two.

The process starts by *adding the biological information to the ASV table*, then diving the dataset by their biological replicate. This will also remove any sample that is not included in the metadata, eg coming from a different project.
** to do: change separate line to get "original sample" and "replicate" columns. 
```{r dissimilarity between PCR replicates}

# ASV.nested %>% 
#   select(Miseq_run, Step3.tibble) %>% 
#   unnest(Step3.tibble) %>%
#   separate(Sample_name, into = "original_sample", sep = "\\.", remove = F) -> cleaned.tibble

ASV.nested %>% 
  select(MiSeqRun, Step2.tibble) %>% 
  unnest(Step2.tibble) %>%
  separate(sample_id, into = c("sample_name_2","original_sample_site", "original_sample_num","replicate"), sep = "-", remove = F) %>%
  unite(col="original_sample",c(original_sample_site,original_sample_num), sep="_",remove=TRUE) %>% dplyr::select(-`sample_name_2`) -> cleaned.tibble
```


```{r quick check}
# do all samples have a name
# cleaned.tibble %>% 
#   filter (sample == "")

# do all of them have an original sample
cleaned.tibble %>% 
  filter(original_sample == "")
# do all of them have a Hash
cleaned.tibble %>% 
  filter(is.na(Hash))
# How many samples, how many Hashes
cleaned.tibble %>%
  ungroup %>% 
  summarise(n_distinct(Sample_name), # 90
            n_distinct(Hash))   # 1677

# Let's check the levels of replication

cleaned.tibble %>% 
  group_by(original_sample) %>% 
  summarise(nrep = n_distinct(Sample_name)) %>% 
  #filter (nrep == 2) # 0
  filter (nrep == 1) # 0 
```


If there are any samples for which we only have 1-2 PCR replicates, get rid of those.

```{r remove single replicates, eval=FALSE}
discard.1 <- cleaned.tibble %>% 
  group_by(original_sample) %>% 
  mutate(nrep = n_distinct(replicate)) %>% 
  #filter (nrep == 2) # 25
  filter (nrep == 1 | nrep == 2) %>% 
  distinct(Sample_name) %>% pull(Sample_name)

cleaned.tibble %>% 
  filter(!Sample_name %in% discard.1) -> cleaned.tibble
```
<br>

And now, let's have a visual representation of the dissimilarities between PCR replicates, biological replicates and everything else. This requires the custom function `tibble_to_matrix`, called above.
```{r lets do the PCR replication}
cleaned.tibble %>%
  group_by (Sample_name, sample_id) %>%
  mutate (Tot = sum(nReads),
          Row.sums = nReads / Tot) %>% 
  group_by (Hash) %>%
  mutate (Colmax = max (Row.sums),
          Normalized.reads = Row.sums / Colmax) -> cleaned.tibble

tibble_to_matrix (cleaned.tibble) -> all.distances.full

# Do all samples have a name?
summary(is.na(names(all.distances.full))) # Yes they do
```
<br>

Let's make the pairwise distances a long table
```{r}
as_tibble(subset(melt(as.matrix(all.distances.full)))) -> all.distances.melted

# Any mjor screw ups? No
summary(is.na(all.distances.melted$value))
```
<br>

Join the distances dataframe to the metadata, so that distances can be grouped as PCR replicates, Biological replicates, or from the same site.
```{r}
all.distances.melted.meta <- all.distances.melted %>%
  left_join(cleaned.tibble %>% 
              ungroup() %>%
              dplyr::select(Sample_name,original_sample) %>%
              distinct(), by=c("Var1"="Sample_name")) %>%
  separate(original_sample, into=c("Site1","Bio1"), remove=FALSE) %>%
  rename("Sample_name1"=Var1, "Site_Bio1"=original_sample) %>%
   left_join(cleaned.tibble %>% 
              ungroup() %>%
              dplyr::select(Sample_name,original_sample) %>%
              distinct(), by=c("Var2"="Sample_name")) %>%
  separate(original_sample, into=c("Site2","Bio2"), remove=FALSE) %>%
  rename("Sample_name2"=Var2, "Site_Bio2"=original_sample)
```
<br>

Now, create a three variables for all distances, they could be PCR replicates, BIOL replicates, or from the same site
```{r}
all.distances.to.plot <- all.distances.melted.meta %>%
  mutate ( Distance.type = case_when( Site_Bio1 == Site_Bio2 ~ "PCR.replicates",
                                      Site1 == Site2 ~ "Same Site",
                                      TRUE ~ "Different Site"
                                     )) %>%
  dplyr::select(Sample_name1, Sample_name2 , value , Distance.type) %>%
  filter (Sample_name1 != Sample_name2)

# Checking all went well

sapply(all.distances.to.plot, function(x) summary(is.na(x))) # good boi
```

```{r}
all.distances.to.plot$Distance.type <- all.distances.to.plot$Distance.type  %>% fct_relevel( "PCR.replicates", "Same Site","Different Site")

  ggplot (all.distances.to.plot ) +
  geom_histogram (aes (fill = Distance.type, x = value, after_stat(ndensity)), position = "dodge",  alpha = 0.9, bins = 50) +
  facet_wrap( ~ Distance.type) +
  labs (x = "Pairwise dissimilarity", y = "density" ,
        Distance.type = "Distance") +
    ggtitle ("Dungeness instars run 1") +
    guides (fill = "none")
ggsave(here(outdir,"visual.anova.png"), dpi = "retina")
```
<br>

So our the distribution of dissimilarities is mostly we expected : lowest in technical replicates, then from individuals at the same site, and higher across our study system. It's interesting that distances between individuals at the same site and different sites are actually pretty similar. 

Now let's see if there are any technical replicates that should be discarded due to their higher dissimilarity. We will calculate the distance from each PCR replicate to their group centroid, fit those distances to a normal distribution and discard values that are too high.

First, calculate distances
```{r}
cleaned.tibble %>% dplyr::select(-MiSeqRun) %>% group_by(original_sample) %>% nest() -> nested.cleaning

nested.cleaning %>% 
  mutate(matrix = map(data, tibble_to_matrix)) -> nested.cleaning
nested.cleaning %>% mutate(ncomparisons = map(matrix, length)) -> nested.cleaning

nested.cleaning <- nested.cleaning %>% mutate (distances = map2(matrix, original_sample, dist_to_centroid))

unlist (nested.cleaning$distances) -> all_distances
```
```{r echo=FALSE}
hist(all_distances)
```
<br>

Then fit a normal distribution and identify outliers. *The outlier distribution is severely skewed, so I'm not sure a normal distribution is the correct one to use*.
```{r}
#normparams <- fitdistr(all_pairwise_distances, "normal")$estimate
normparams <- MASS::fitdistr(all_distances, "normal")$estimate                                      
#  probs <- pnorm(all_pairwise_distances, normparams[1], normparams[2])
probs <- pnorm(all_distances, normparams[1], normparams[2])
outliers <- which(probs>0.95)

discard <-names (all_distances[outliers])
print(discard)
```

Write out the discarded samples. *I don't have any*
```{r}
to_write_discarded <- as.data.frame(all_distances[outliers]) %>% rownames_to_column("Sample_name") %>% dplyr::select(Sample_name, 
                                                                                                     distance_to_centroid = 2)
to_write_discarded <- to_write_discarded %>% bind_rows(tibble(Sample_name = discard,
                                                              distance_to_centroid = NA))
write_csv(to_write_discarded ,here(outdir,"discarded_samples.csv"))
```
<br>

Finally, let's remove these samples from the dataset
```{r actual cleaning}
ASV.nested %>% 
  mutate(Step4.tibble = map (Step2.tibble,  ~ filter(.,! Sample_name %in% to_write_discarded$sample))) -> ASV.nested

ASV.nested %>% 
  transmute(Summary.1 = map(Step4.tibble, ~ how.many(ASVtable = .,round = "5.PCR.dissimilarity"))) %>% 
  left_join(ASV.summary) %>% 
  mutate(Summary   = map2(Summary, Summary.1, bind_rows)) %>%
  dplyr::select(-Summary.1) -> ASV.summary 
```


## Exporting the output

We will export the final cleaned table with four columns (Miseq_run, sample, Hash, nReads)

```{r}
ASV.nested %>% 
  select(MiSeqRun, Step4.tibble) %>% 
  unnest(Step4.tibble) %>% 
  mutate(nReads = as.integer(nReads)) %>% 
  write_csv(here(outdir,"ASV_table_clean.csv"))

ASV.nested %>% 
  select(Step4.tibble) %>% 
  unnest(Step4.tibble) %>% 
  distinct(Hash) %>% 
  left_join(all.hashes) %>% 
  write_csv(here(outdir,"Hash_Key_clean.csv"))



input <- read_csv(here(dada2_dir,"Hash_key_clean.csv"))

write.fasta (sequences = as.list(input$Sequence),
             names = as.list(input$Hash),
             file.out = here(outdir,"Hash_Key_clean.fasta"))


```


**For diet analysis**, we might expect a high proportion of prey items to be rare in the data, assuming the primer has a strong affinity for the predator DNA. This means a high proportion of "drop-outs" across PCR replicates, and therefore higher distance values across replicates. To be safe, let's write out the hashes from Step 2. We don't need to write out the hash key again.
```{r}
ASV.nested %>% 
  select(MiSeqRun, Step2.tibble) %>% 
  unnest(Step2.tibble) %>% 
  mutate(nReads = as.integer(nReads)) %>% 
  write_csv(here(outdir,"ASV_table_step2clean.csv"))

ASV.nested %>% 
  select(Step2.tibble) %>% 
  unnest(Step2.tibble) %>%
  distinct(Hash) %>% 
  left_join(all.hashes) %>% 
  write_csv(here(outdir,"Hash_Key_step2clean.csv"))

input <- read_csv(here(dada2_dir,"Hash_key_step2clean.csv"))

write.fasta (sequences = as.list(input$Sequence),
             names = as.list(input$Hash),
             file.out = here(outdir,"Hash_Key_step2clean.fasta"))
```

<br>


## Summary of the cleanup process

```{r last graph}

ASV.summary %>% 
  unnest(cols=c(Summary)) %>% 
  ggplot(aes(x=Stage, y=number, fill = Stat))+
    geom_line(aes(group = MiSeqRun, color = MiSeqRun))+
  facet_grid(Stat~., scales = "free")+
  theme(axis.text.x = element_text(angle = 45, hjust =1),
        legend.position="none")
                                 

ggsave(here(outdir,"denoising_reads_per_step.png"), dpi = "retina")

```

# Coverage Table with nReads

```{r}
# Vector with desired order

order.Sites <- c("CLAY", "KAY", "MARPT")

filtered.ASV <- ASV.nested %>% 
  select(Step4.tibble) %>% 
  unnest(Step4.tibble) %>% 
  separate(sample_id, into = c("sample_name_y","site", "sample_num","replicate"), sep = "-", remove = F) %>%
  unite(col="sample",c(site,sample_num), sep="_",remove=FALSE) %>% dplyr::select(-`sample_name_y`) %>%
  mutate(nReads = as.integer(nReads)) %>%
  group_by(sample, site) %>%
  summarise (nReads = sum(nReads))
    

filtered.ASV %>% 
  mutate(site = factor(site, levels = order.Sites)) %>% 
  ggplot(aes(x = site, 
             y = nReads/1000)) +
  geom_boxplot(aes(group=site),outlier.alpha = 0.5) +
  geom_point() +
  labs(y = "# reads (x1000)", x = "") +
  theme_bw() +
  theme(legend.position = c(0.6, 0.2)) 
ggsave(here(outdir,"denoising_reads_by_site.png"))
```
<br>

The graph can also be written out as a table:
```{r eval=FALSE}
filtered.ASV %>% 
  group_by(site) %>% 
  summarise (mean = mean(nReads), max = max(nReads), min = min(nReads), sd = sd(nReads)) %>% 
transmute (site, data = paste0("(", round(mean,0), " +- ", round(sd,0), ")")) %>% 
  right_join(Coverage.dataset) %>% 
  mutate( data = paste0(cases, " ", data)) %>% 
  arrange(Date) %>% 
  mutate (Date = paste(lubridate::month(Date, label = T),
                      lubridate::year(Date),
                      sep= "'")) %>% 
  pivot_wider(names_from = Date, values_from = data, id_cols = c(Area, Site),values_fill = list(data = "") ) %>% 
  slice(match(Site, order.Sites)) %>% 
  kableExtra::kable() %>% 
  kableExtra::kable_styling() %>% 
  kableExtra::collapse_rows(1, valign = "top") 
```
