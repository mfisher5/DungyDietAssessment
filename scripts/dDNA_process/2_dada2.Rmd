---
title: "DADA2"
subtitle: "last knit: `r format(Sys.time(), '%B %d, %Y')`"
author: "M Fisher (from Eily, Moncho)"
date: '2022-09-30'
output: 
  html_document:
    toc: yes
---


# Description

Run **DADA2** [tutorial here](https://benjjneb.github.io/dada2/tutorial.html) in order to get an amplicon sequence variant (ASV) table, which records the number of times each exact amplicon sequence variant was observed in each sample. 

Certain decisions have to be made throughout the script, so *do not just knit this script with the existing values*. Go through each code chunk in R first, then knit. Certain code chunks will not re-run when the script is knitted, to avoid over-writing existing files.

Input: demultiplexed fastq files, without barcodes / adapters / primers. 



<br>

# Set up

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require("here")) {install.packages("here")}
if(!require("tidyverse")) {install.packages("tidyverse")}
if(!require("magrittr")) {install.packages("magrittr")}
if(!require("digest")) {install.packages("digest")}
if(!require("seqinr")) {install.packages("seqinr")}
# if(!requireNamespace("BiocManager", quietly = TRUE)){install.packages("BiocManager")}
# BiocManager::install("dada2", version = "3.10")
library(dada2)
library(digest)
library(seqinr)
```
<br>

User directories
```{r set up }
# metadata directory
metadat_dir <- "data/metadata"
# root directory for cutadapt
cutadapt_dir <- "data/cutadapt/run_2"
# output directory
outdir <- "data/dada2/run_2"
```

User inputs
```{r}
run.num = 2

hash = TRUE  # rarely do you want hash = false (EA)

keep.mid.files = FALSE # I find I never look at these / use these and they just take up space (EA)
```
<br>
<br>


# Prep for DADA2

```{r message=FALSE, warning=FALSE}
run_cutadapt_dir = paste0(cutadapt_dir, "/noprimers")
```
<br>

read in sequencing metadata. set default trim length based on primer. 
```{r message=FALSE}
cutadapt.meta <- read_csv(here(run_cutadapt_dir, paste0("output.metadata.csv")))
marker        <- unique(cutadapt.meta$Locus)
print(marker)
```
```{r echo=FALSE}
if(marker=="Leray" | marker=="LerayXT"){
  trimming.length.r1 = 250
  trimming.length.r2 = 200
  message("trim lengths set as (r1,r1): ", trimming.length.r1, ",",trimming.length.r2)
} else if(marker=="BF3"){
  trimming.length.r1 = 260
  trimming.length.r2 = 200
} else{
  message("please manually enter trim length for this marker.")
}
```
<br>

read in file names.
```{r}
fnFs <- sort(list.files(path=here(run_cutadapt_dir), pattern="_R1_001.fastq.fastq", full.names = TRUE))
fnFs_simple <- str_remove(fnFs,pattern=paste0(here(run_cutadapt_dir),"/"))
fnRs <- sort(list.files(path=here(run_cutadapt_dir), pattern="_R2_001.fastq.fastq", full.names = TRUE))
```
<br>

make sure that all of the samples run through cutadapt were retained after trimming. 
```{r}
file_diff <- fnFs_simple[which(!(cutadapt.meta$file1 %in% fnFs_simple))]

if(length(file_diff)>0){
  cutadapt.meta <- filter(cutadapt.meta, file1 %in% fnFs_pathless)
  message("removed the following samples, which have no data:")
  print(file_diff)
}
```
<br>

remove any files from the cutadapt metadata that don't have corresponding files in the post-cutadapt folder (the indexes that weren't used in the run)
```{r}
cutadapt.meta.output <- filter(cutadapt.meta, file1 %in% fnFs_simple)
```
<br>

**for run 2: remove the "NONE" files. none of the reads pass filter.**
```{r}
if(run.num==2){
  cutadapt.meta.output %<>% filter(!(grepl("NONE", file1)))
}
```


get the sample names, which will be used to name the filtered files.
```{r}
if(run.num==1){
# sample.names <- marker.meta$Sample_name
prefix <- paste0("Locus_",marker,"_")
suffix <- paste0("_L001_R1_001.fastq.fastq")
sample.names.df <- (cutadapt.meta.output %>% dplyr::select(file1) %>%
                      filter(file1 %in% fnFs_simple) %>%
                    mutate(sample_id=str_remove(file1,prefix)) %>%
                      mutate(sample_id=str_remove(sample_id,suffix)) %>%
                      separate(col=sample_id, into=c("sample_id","sample.num"), sep="_S")) %>% dplyr::select(sample_id)
sample.names <- as.character(sample.names.df$sample_id)
} else if (run.num==2){
  sample.names <- read_csv(here('data','DCRB_Run2_samples.csv')) %>% 
    filter(!(grepl("NONE", sample_label)) & !(grepl("Control",sample_label))) %>% pull(sample_label)
  sample.names <- c(sample.names,"NegativeControl","PositiveControl")
  length(sample.names)==dim(cutadapt.meta.output)[1]
}
```
<br>


write output directory path for filtered files in the run's cutadapt folder.
```{r}
filt.dir <- paste0(cutadapt_dir, "/noprimers_filtered")
```
<br>

create directories if they don't exist
```{r}
if(!dir.exists(here(filt.dir))){
  dir.create(path = here(filt.dir),recursive = T)
}
if(!dir.exists(here(outdir))){
  dir.create(path = here(outdir),recursive = T)
}
```
<br>

Double check the quality scores, **if not already completed in QC script 0**
Plot forward read quality scores, one sample per group
```{r eval=FALSE}
plotQualityProfile(fnFs[c(1,2,6,19,25)])
```
<br>

Plot forward read quality scores, one sample per group
```{r eval=FALSE}
plotQualityProfile(fnRs[c(1,2,6,19,25)])
```
<br>


manually enter trim lengths - if there isn't a default for the marker, or if the defaults are too long based on the quality scores (see script 0_qc)
```{r eval=FALSE}
trimming.length.r1 = 250
trimming.length.r2 = 200
```
<br>
<br>

# DADA2

## Filter and trim

- `truncLen` truncates the sequence length, and should be based on per-base quality scores. I'm using the length that Eily suggested , 120bp.
- `maxEE` discards reads with high "expected errors" = $\sum(10^{-Q/10})$
- `rm.phix` removes reads from PhiX spike
- use `compress` to gzip the output fastqs

```{r}
output.dada2 <- cutadapt.meta.output %>%
  #filter(rc == 1) %>% # ONLY SELECT THE BACKWARDS ONES (1) OR FORWARDS ONES (0)
  mutate(basename=sample.names) %>%
  mutate(file1  = here(run_cutadapt_dir, file1),
         file2  = here(run_cutadapt_dir, file2),
         filtF1 = here(filt.dir, paste0(basename, "_F1_filt.fastq.gz")),
         filtR1 = here(filt.dir, paste0(basename, "_R1_filt.fastq.gz"))) %>%
  select(-basename) %>% 
  mutate (outFs = pmap(.l= list (file1, filtF1, file2, filtR1),
                       .f = function(a, b, c, d) {
                         filterAndTrim(a,b,c,d,
                                       truncLen=c(trimming.length.r1,trimming.length.r2),
                                       maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                                       compress=TRUE, multithread=FALSE )
                       } ),
          errF1 = map(filtF1, ~ learnErrors(.x, multithread=FALSE,verbose = 0)),     # Calculate errors
          errR1 = map(filtR1, ~ learnErrors(.x, multithread=FALSE,verbose = 0)),
          derepF1 = map(filtF1, derepFastq),                   # dereplicate seqs
          derepR1 = map(filtR1, derepFastq),
          dadaF1  = map2(derepF1,errF1, ~ dada(.x, err = .y, multithread = FALSE)),  # dada2
          dadaR1  = map2(derepR1,errR1, ~ dada(.x, err = .y, multithread = FALSE)),
          mergers = pmap(.l = list(dadaF1,derepF1, dadaR1,derepR1),                 # merge things
                         .f = mergePairs ))

if (keep.mid.files==TRUE){
  write_rds(output.dada2, path = here(outdir, "output.halfway.rds"))}
```
Sample 1 - 176269 reads in 13513 unique sequences.
Sample 1 - 186720 reads in 12866 unique sequences.
Sample 1 - 220428 reads in 14532 unique sequences.
Sample 1 - 137377 reads in 19566 unique sequences.
Sample 1 - 172368 reads in 22327 unique sequences.
Sample 1 - 186817 reads in 24612 unique sequences.
Sample 1 - 52141 reads in 5011 unique sequences.
Sample 1 - 72836 reads in 6509 unique sequences.
Sample 1 - 160955 reads in 11378 unique sequences.
Sample 1 - 232423 reads in 13558 unique sequences.
Sample 1 - 109892 reads in 9890 unique sequences.
Sample 1 - 123853 reads in 11515 unique sequences.
Sample 1 - 28525 reads in 4361 unique sequences.
Sample 1 - 18739 reads in 3094 unique sequences.
Sample 1 - 5844 reads in 1229 unique sequences.
Sample 1 - 217270 reads in 27800 unique sequences.
Sample 1 - 251522 reads in 31134 unique sequences.
Sample 1 - 225438 reads in 26201 unique sequences.
Sample 1 - 184215 reads in 22235 unique sequences.
Sample 1 - 139183 reads in 20789 unique sequences.
Sample 1 - 176752 reads in 22017 unique sequences.
Sample 1 - 91214 reads in 10722 unique sequences.
Sample 1 - 184241 reads in 19666 unique sequences.
Sample 1 - 164785 reads in 17443 unique sequences.
Sample 1 - 261581 reads in 30094 unique sequences.
Sample 1 - 148850 reads in 17239 unique sequences.
Sample 1 - 323767 reads in 34021 unique sequences.
Sample 1 - 212000 reads in 20832 unique sequences.
Sample 1 - 200597 reads in 20600 unique sequences.
Sample 1 - 181109 reads in 17517 unique sequences.
Sample 1 - 167261 reads in 24169 unique sequences.
Sample 1 - 186867 reads in 18221 unique sequences.
Sample 1 - 203053 reads in 19160 unique sequences.
Sample 1 - 218980 reads in 23434 unique sequences.
Sample 1 - 261839 reads in 28307 unique sequences.
Sample 1 - 223980 reads in 21814 unique sequences.
Sample 1 - 172974 reads in 29632 unique sequences.
Sample 1 - 164833 reads in 27230 unique sequences.
Sample 1 - 199381 reads in 28204 unique sequences.
Sample 1 - 245653 reads in 20160 unique sequences.
Sample 1 - 223353 reads in 18923 unique sequences.
Sample 1 - 224495 reads in 19734 unique sequences.
Sample 1 - 154511 reads in 21228 unique sequences.
Sample 1 - 154331 reads in 23753 unique sequences.
Sample 1 - 147926 reads in 18360 unique sequences.
Sample 1 - 141334 reads in 10659 unique sequences.
Sample 1 - 134118 reads in 11291 unique sequences.
Sample 1 - 178967 reads in 12658 unique sequences.
Sample 1 - 119612 reads in 9865 unique sequences.
Sample 1 - 72720 reads in 7078 unique sequences.
Sample 1 - 37327 reads in 4103 unique sequences.
Sample 1 - 270588 reads in 17163 unique sequences.
Sample 1 - 241365 reads in 17364 unique sequences.
Sample 1 - 269831 reads in 17449 unique sequences.
Sample 1 - 223688 reads in 11635 unique sequences.
Sample 1 - 184081 reads in 10256 unique sequences.
Sample 1 - 191891 reads in 10073 unique sequences.
Sample 1 - 76183 reads in 7680 unique sequences.
Sample 1 - 197533 reads in 16908 unique sequences.
Sample 1 - 233064 reads in 19058 unique sequences.
Sample 1 - 55186 reads in 4629 unique sequences.
Sample 1 - 187420 reads in 12348 unique sequences.
Sample 1 - 147230 reads in 14758 unique sequences.
Sample 1 - 314207 reads in 24643 unique sequences.
Sample 1 - 220035 reads in 19584 unique sequences.
Sample 1 - 268527 reads in 22525 unique sequences.
Sample 1 - 101283 reads in 6978 unique sequences.
Sample 1 - 17382 reads in 2387 unique sequences.
Sample 1 - 30562 reads in 2339 unique sequences.
Sample 1 - 79506 reads in 5065 unique sequences.
Sample 1 - 167954 reads in 9594 unique sequences.
Sample 1 - 157712 reads in 8320 unique sequences.
Sample 1 - 87170 reads in 5718 unique sequences.
Sample 1 - 226828 reads in 11239 unique sequences.
Sample 1 - 148936 reads in 7110 unique sequences.
Sample 1 - 134817 reads in 14673 unique sequences.
Sample 1 - 140179 reads in 14792 unique sequences.
Sample 1 - 202916 reads in 13341 unique sequences.
Sample 1 - 29125 reads in 2080 unique sequences.
Sample 1 - 53227 reads in 3035 unique sequences.
Sample 1 - 68574 reads in 3658 unique sequences.
Sample 1 - 369556 reads in 29830 unique sequences.
Sample 1 - 393294 reads in 31310 unique sequences.
Sample 1 - 427837 reads in 35807 unique sequences.
Sample 1 - 6495 reads in 837 unique sequences.
Sample 1 - 74022 reads in 7729 unique sequences.
Sample 1 - 53119 reads in 4347 unique sequences.
Sample 1 - 200216 reads in 11753 unique sequences.
Sample 1 - 114973 reads in 12479 unique sequences.
Sample 1 - 132229 reads in 8624 unique sequences.
Sample 1 - 4069 reads in 720 unique sequences.
Sample 1 - 29 reads in 16 unique sequences.
Sample 1 - 176269 reads in 18052 unique sequences.
Sample 1 - 186720 reads in 17532 unique sequences.
Sample 1 - 220428 reads in 16484 unique sequences.
Sample 1 - 137377 reads in 28329 unique sequences.
Sample 1 - 172368 reads in 25701 unique sequences.
Sample 1 - 186817 reads in 27882 unique sequences.
Sample 1 - 52141 reads in 5626 unique sequences.
Sample 1 - 72836 reads in 8420 unique sequences.
Sample 1 - 160955 reads in 16810 unique sequences.
Sample 1 - 232423 reads in 21598 unique sequences.
Sample 1 - 109892 reads in 10267 unique sequences.
Sample 1 - 123853 reads in 13065 unique sequences.
Sample 1 - 28525 reads in 3717 unique sequences.
Sample 1 - 18739 reads in 3519 unique sequences.
Sample 1 - 5844 reads in 1125 unique sequences.
Sample 1 - 217270 reads in 32127 unique sequences.
Sample 1 - 251522 reads in 30163 unique sequences.
Sample 1 - 225438 reads in 29203 unique sequences.
Sample 1 - 184215 reads in 23548 unique sequences.
Sample 1 - 139183 reads in 19616 unique sequences.
Sample 1 - 176752 reads in 20463 unique sequences.
Sample 1 - 91214 reads in 11080 unique sequences.
Sample 1 - 184241 reads in 21213 unique sequences.
Sample 1 - 164785 reads in 18864 unique sequences.
Sample 1 - 261581 reads in 33668 unique sequences.
Sample 1 - 148850 reads in 24612 unique sequences.
Sample 1 - 323767 reads in 37269 unique sequences.
Sample 1 - 212000 reads in 23885 unique sequences.
Sample 1 - 200597 reads in 21566 unique sequences.
Sample 1 - 181109 reads in 20935 unique sequences.
Sample 1 - 167261 reads in 21953 unique sequences.
Sample 1 - 186867 reads in 22591 unique sequences.
Sample 1 - 203053 reads in 25092 unique sequences.
Sample 1 - 218980 reads in 29934 unique sequences.
Sample 1 - 261839 reads in 31140 unique sequences.
Sample 1 - 223980 reads in 24535 unique sequences.
Sample 1 - 172974 reads in 27031 unique sequences.
Sample 1 - 164833 reads in 25906 unique sequences.
Sample 1 - 199381 reads in 32270 unique sequences.
Sample 1 - 245653 reads in 23374 unique sequences.
Sample 1 - 223353 reads in 23982 unique sequences.
Sample 1 - 224495 reads in 22622 unique sequences.
Sample 1 - 154511 reads in 18345 unique sequences.
Sample 1 - 154331 reads in 16922 unique sequences.
Sample 1 - 147926 reads in 19913 unique sequences.
Sample 1 - 141334 reads in 12993 unique sequences.
Sample 1 - 134118 reads in 13934 unique sequences.
Sample 1 - 178967 reads in 15210 unique sequences.
Sample 1 - 119612 reads in 13197 unique sequences.
Sample 1 - 72720 reads in 8052 unique sequences.
Sample 1 - 37327 reads in 4329 unique sequences.
Sample 1 - 270588 reads in 22461 unique sequences.
Sample 1 - 241365 reads in 18675 unique sequences.
Sample 1 - 269831 reads in 19102 unique sequences.
Sample 1 - 223688 reads in 15156 unique sequences.
Sample 1 - 184081 reads in 17000 unique sequences.
Sample 1 - 191891 reads in 13637 unique sequences.
Sample 1 - 76183 reads in 8711 unique sequences.
Sample 1 - 197533 reads in 19181 unique sequences.
Sample 1 - 233064 reads in 23498 unique sequences.
Sample 1 - 55186 reads in 6906 unique sequences.
Sample 1 - 187420 reads in 16393 unique sequences.
Sample 1 - 147230 reads in 10773 unique sequences.
Sample 1 - 314207 reads in 35830 unique sequences.
Sample 1 - 220035 reads in 26299 unique sequences.
Sample 1 - 268527 reads in 30409 unique sequences.
Sample 1 - 101283 reads in 6539 unique sequences.
Sample 1 - 17382 reads in 1505 unique sequences.
Sample 1 - 30562 reads in 2987 unique sequences.
Sample 1 - 79506 reads in 6403 unique sequences.
Sample 1 - 167954 reads in 12805 unique sequences.
Sample 1 - 157712 reads in 14568 unique sequences.
Sample 1 - 87170 reads in 6680 unique sequences.
Sample 1 - 226828 reads in 13315 unique sequences.
Sample 1 - 148936 reads in 12203 unique sequences.
Sample 1 - 134817 reads in 10908 unique sequences.
Sample 1 - 140179 reads in 10269 unique sequences.
Sample 1 - 202916 reads in 17307 unique sequences.
Sample 1 - 29125 reads in 3300 unique sequences.
Sample 1 - 53227 reads in 5917 unique sequences.
Sample 1 - 68574 reads in 4954 unique sequences.
Sample 1 - 369556 reads in 41499 unique sequences.
Sample 1 - 393294 reads in 40545 unique sequences.
Sample 1 - 427837 reads in 54199 unique sequences.
Sample 1 - 6495 reads in 1065 unique sequences.
Sample 1 - 74022 reads in 5699 unique sequences.
Sample 1 - 53119 reads in 5402 unique sequences.
Sample 1 - 200216 reads in 26185 unique sequences.
Sample 1 - 114973 reads in 7279 unique sequences.
Sample 1 - 132229 reads in 10206 unique sequences.
Sample 1 - 4069 reads in 1430 unique sequences.
Sample 1 - 29 reads in 13 unique sequences.

**Run 2:**
Sample 1 - 18287 reads in 3453 unique sequences.
Sample 1 - 61271 reads in 7859 unique sequences.
Sample 1 - 179199 reads in 20780 unique sequences.
Sample 1 - 156873 reads in 19375 unique sequences.
Sample 1 - 164957 reads in 16184 unique sequences.
Sample 1 - 138133 reads in 13053 unique sequences.
Sample 1 - 183255 reads in 13004 unique sequences.
Sample 1 - 153000 reads in 9568 unique sequences.
Sample 1 - 195588 reads in 12747 unique sequences.
Sample 1 - 176434 reads in 17711 unique sequences.
Sample 1 - 176215 reads in 18861 unique sequences.
Sample 1 - 179851 reads in 19022 unique sequences.
Sample 1 - 186139 reads in 13740 unique sequences.
Sample 1 - 158609 reads in 12028 unique sequences.
Sample 1 - 128741 reads in 9864 unique sequences.
Sample 1 - 101087 reads in 10139 unique sequences.
Sample 1 - 180736 reads in 15970 unique sequences.
Sample 1 - 134265 reads in 12954 unique sequences.
Sample 1 - 84613 reads in 5478 unique sequences.
Sample 1 - 129786 reads in 8268 unique sequences.
Sample 1 - 55497 reads in 4279 unique sequences.
Sample 1 - 94601 reads in 15135 unique sequences.
Sample 1 - 147459 reads in 23347 unique sequences.
Sample 1 - 109266 reads in 17503 unique sequences.
Sample 1 - 89289 reads in 8273 unique sequences.
Sample 1 - 131593 reads in 14480 unique sequences.
Sample 1 - 117734 reads in 10491 unique sequences.
Sample 1 - 8796 reads in 1123 unique sequences.
Sample 1 - 72437 reads in 5675 unique sequences.
Sample 1 - 94622 reads in 5822 unique sequences.
Sample 1 - 129695 reads in 17363 unique sequences.
Sample 1 - 130353 reads in 13791 unique sequences.
Sample 1 - 124117 reads in 13416 unique sequences.
Sample 1 - 17358 reads in 1350 unique sequences.
Sample 1 - 59326 reads in 2967 unique sequences.
Sample 1 - 59326 reads in 2967 unique sequences.
Sample 1 - 137336 reads in 10485 unique sequences.
Sample 1 - 155882 reads in 11966 unique sequences.
Sample 1 - 143256 reads in 10891 unique sequences.
Sample 1 - 124086 reads in 11947 unique sequences.
Sample 1 - 152629 reads in 14211 unique sequences.
Sample 1 - 112883 reads in 10328 unique sequences.
Sample 1 - 31406 reads in 3028 unique sequences.
Sample 1 - 111167 reads in 7970 unique sequences.
Sample 1 - 127103 reads in 8471 unique sequences.
Sample 1 - 121234 reads in 15805 unique sequences.
Sample 1 - 143172 reads in 22912 unique sequences.
Sample 1 - 136231 reads in 18743 unique sequences.
Sample 1 - 185291 reads in 22567 unique sequences.
Sample 1 - 142262 reads in 20780 unique sequences.
Sample 1 - 95163 reads in 12446 unique sequences.
Sample 1 - 145654 reads in 20594 unique sequences.
Sample 1 - 190415 reads in 27476 unique sequences.
Sample 1 - 104624 reads in 16459 unique sequences.
Sample 1 - 9257 reads in 1325 unique sequences.
Sample 1 - 12038 reads in 1581 unique sequences.
Sample 1 - 7513 reads in 1194 unique sequences.
Sample 1 - 170491 reads in 18105 unique sequences.
Sample 1 - 208745 reads in 21245 unique sequences.
Sample 1 - 197927 reads in 20725 unique sequences.
Sample 1 - 120677 reads in 15175 unique sequences.
Sample 1 - 154181 reads in 19741 unique sequences.
Sample 1 - 156841 reads in 17863 unique sequences.
Sample 1 - 197018 reads in 21395 unique sequences.
Sample 1 - 152572 reads in 17340 unique sequences.
Sample 1 - 150544 reads in 15509 unique sequences.
Sample 1 - 123295 reads in 15624 unique sequences.
Sample 1 - 139093 reads in 20673 unique sequences.
Sample 1 - 135916 reads in 15579 unique sequences.
Sample 1 - 231016 reads in 22260 unique sequences.
Sample 1 - 177673 reads in 16699 unique sequences.
Sample 1 - 242196 reads in 22682 unique sequences.
Sample 1 - 67620 reads in 7837 unique sequences.
Sample 1 - 144413 reads in 15269 unique sequences.
Sample 1 - 149005 reads in 15291 unique sequences.
Sample 1 - 109787 reads in 10783 unique sequences.
Sample 1 - 119518 reads in 13673 unique sequences.
Sample 1 - 100039 reads in 9851 unique sequences.
Sample 1 - 102965 reads in 11312 unique sequences.
Sample 1 - 90708 reads in 11456 unique sequences.
Sample 1 - 126967 reads in 15015 unique sequences.
Sample 1 - 139984 reads in 15251 unique sequences.
Sample 1 - 107299 reads in 10038 unique sequences.
Sample 1 - 150827 reads in 16377 unique sequences.
Sample 1 - 173497 reads in 14680 unique sequences.
Sample 1 - 150156 reads in 15427 unique sequences.
Sample 1 - 163664 reads in 12962 unique sequences.
Sample 1 - 130686 reads in 11217 unique sequences.
Sample 1 - 149175 reads in 13939 unique sequences.
Sample 1 - 103432 reads in 10249 unique sequences.
Sample 1 - 1922 reads in 694 unique sequences.
Sample 1 - 165216 reads in 15398 unique sequences.
Sample 1 - 18287 reads in 2237 unique sequences.
Sample 1 - 61271 reads in 6340 unique sequences.
Sample 1 - 179199 reads in 14363 unique sequences.
Sample 1 - 156873 reads in 14589 unique sequences.
Sample 1 - 164957 reads in 15271 unique sequences.
Sample 1 - 138133 reads in 12261 unique sequences.
Sample 1 - 183255 reads in 12781 unique sequences.
Sample 1 - 153000 reads in 10844 unique sequences.
Sample 1 - 195588 reads in 11759 unique sequences.
Sample 1 - 176434 reads in 15070 unique sequences.
Sample 1 - 176215 reads in 14054 unique sequences.
Sample 1 - 179851 reads in 16058 unique sequences.
Sample 1 - 186139 reads in 11379 unique sequences.
Sample 1 - 158609 reads in 8975 unique sequences.
Sample 1 - 128741 reads in 8067 unique sequences.
Sample 1 - 101087 reads in 9863 unique sequences.
Sample 1 - 180736 reads in 17234 unique sequences.
Sample 1 - 134265 reads in 12658 unique sequences.
Sample 1 - 84613 reads in 4689 unique sequences.
Sample 1 - 129786 reads in 6495 unique sequences.
Sample 1 - 55497 reads in 3212 unique sequences.
Sample 1 - 94601 reads in 10374 unique sequences.
Sample 1 - 147459 reads in 19523 unique sequences.
Sample 1 - 109266 reads in 12876 unique sequences.
Sample 1 - 89289 reads in 6157 unique sequences.
Sample 1 - 131593 reads in 8252 unique sequences.
Sample 1 - 117734 reads in 8301 unique sequences.
Sample 1 - 8796 reads in 804 unique sequences.
Sample 1 - 72437 reads in 3718 unique sequences.
Sample 1 - 94622 reads in 4732 unique sequences.
Sample 1 - 129695 reads in 11095 unique sequences.
Sample 1 - 130353 reads in 11479 unique sequences.
Sample 1 - 124117 reads in 10433 unique sequences.
Sample 1 - 17358 reads in 1051 unique sequences.
Sample 1 - 59326 reads in 2539 unique sequences.
Sample 1 - 59326 reads in 2539 unique sequences.
Sample 1 - 137336 reads in 7550 unique sequences.
Sample 1 - 155882 reads in 9337 unique sequences.
Sample 1 - 143256 reads in 8278 unique sequences.
Sample 1 - 124086 reads in 11112 unique sequences.
Sample 1 - 152629 reads in 13316 unique sequences.
Sample 1 - 112883 reads in 10433 unique sequences.
Sample 1 - 31406 reads in 2085 unique sequences.
Sample 1 - 111167 reads in 5637 unique sequences.
Sample 1 - 127103 reads in 6159 unique sequences.
Sample 1 - 121234 reads in 14066 unique sequences.
Sample 1 - 143172 reads in 14988 unique sequences.
Sample 1 - 136231 reads in 16812 unique sequences.
Sample 1 - 185291 reads in 21605 unique sequences.
Sample 1 - 142262 reads in 14062 unique sequences.
Sample 1 - 95163 reads in 12071 unique sequences.
Sample 1 - 145654 reads in 15670 unique sequences.
Sample 1 - 190415 reads in 19273 unique sequences.
Sample 1 - 104624 reads in 10077 unique sequences.
Sample 1 - 9257 reads in 1006 unique sequences.
Sample 1 - 12038 reads in 1037 unique sequences.
Sample 1 - 7513 reads in 733 unique sequences.
Sample 1 - 170491 reads in 13344 unique sequences.
Sample 1 - 208745 reads in 16138 unique sequences.
Sample 1 - 197927 reads in 13941 unique sequences.
Sample 1 - 120677 reads in 11936 unique sequences.
Sample 1 - 154181 reads in 15103 unique sequences.
Sample 1 - 156841 reads in 14895 unique sequences.
Sample 1 - 197018 reads in 16052 unique sequences.
Sample 1 - 152572 reads in 11165 unique sequences.
Sample 1 - 150544 reads in 11414 unique sequences.
Sample 1 - 123295 reads in 11774 unique sequences.
Sample 1 - 139093 reads in 11590 unique sequences.
Sample 1 - 135916 reads in 12421 unique sequences.
Sample 1 - 231016 reads in 15010 unique sequences.
Sample 1 - 177673 reads in 13139 unique sequences.
Sample 1 - 242196 reads in 16507 unique sequences.
Sample 1 - 67620 reads in 5664 unique sequences.
Sample 1 - 144413 reads in 11618 unique sequences.
Sample 1 - 149005 reads in 10423 unique sequences.
Sample 1 - 109787 reads in 7901 unique sequences.
Sample 1 - 119518 reads in 8300 unique sequences.
Sample 1 - 100039 reads in 7564 unique sequences.
Sample 1 - 102965 reads in 8899 unique sequences.
Sample 1 - 90708 reads in 6956 unique sequences.
Sample 1 - 126967 reads in 10103 unique sequences.
Sample 1 - 139984 reads in 11475 unique sequences.
Sample 1 - 107299 reads in 9568 unique sequences.
Sample 1 - 150827 reads in 10768 unique sequences.
Sample 1 - 173497 reads in 10827 unique sequences.
Sample 1 - 150156 reads in 7977 unique sequences.
Sample 1 - 163664 reads in 9334 unique sequences.
Sample 1 - 130686 reads in 8788 unique sequences.
Sample 1 - 149175 reads in 9463 unique sequences.
Sample 1 - 103432 reads in 6346 unique sequences.
Sample 1 - 1922 reads in 570 unique sequences.
Sample 1 - 165216 reads in 17718 unique sequences.


<br>

The sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. 
```{r}
seqtab <- makeSequenceTable(output.dada2$mergers)
dim(seqtab)
```
92 | 2749
92 | 2800
<br>

Inspect distribution of sequence lengths:
```{r echo=FALSE}
table(nchar(getSequences(seqtab)))

table(nchar(getSequences(seqtab))) %>% as.data.frame() %>%
  mutate(Length=as.character(Var1),
         Length=as.numeric(Length)) %>%
  ggplot( aes(x=Length,y=Freq)) +
  geom_col() + theme_bw()
```

65% are 313 (yay!) with some larger (quite a few at 352) and some shorter (about 100 at 304)

```{r}
write.csv(table(nchar(getSequences(seqtab))) %>% as.data.frame() %>%
  mutate(Length=as.character(Var1),
         Length=as.numeric(Length)), file=here(outdir,'dada2_filtered_seqLengths.csv'))
```
<br>


## Remove chimeras

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", verbose=TRUE)
dim(seqtab.nochim)

seqtab.nochim.df <- as.data.frame(seqtab.nochim)
```
Identified 1067 bimeras out of 2749 input sequences.
[1]   92 | 1682

weird, that's a higher proportion of bimeras than I'm used to seeing. 
Identified 539 bimeras out of 2800 input sequences.
[1]   92 2261

<br>

## Write output

Copy the metadata so it is all in one place
```{r}
cutadapt.meta.output %>% write_csv(here(outdir,"dada2.metadata.csv"))
```
<br>

Output file names
```{r}
conv_file <- here(outdir,"hash_key.csv")
conv_file.fasta <- here(outdir,"hash_key.fasta")
ASV_file <-  here(outdir,"ASV_table.csv")
```
<br>

If using hashes, set up the output table with hash IDs and write it out.
```{r}
if (hash==TRUE)
{conv_table <- tibble( Hash = "", Sequence ="")
  map_chr (colnames(seqtab.nochim.df), ~ digest(.x, algo = "sha1", serialize = F, skip = "auto")) -> Hashes
  conv_table <- tibble (Hash = Hashes,
                        Sequence = colnames(seqtab.nochim.df))
  seqtab.nochim.hashes.df <- seqtab.nochim.df
  colnames(seqtab.nochim.hashes.df) <- Hashes

  write_csv(conv_table, conv_file) # write the table into a file
  write.fasta(sequences = as.list(conv_table$Sequence),
              names     = as.list(conv_table$Hash),
              file.out = conv_file.fasta)
  seqtab.nochim.hashes.df <- bind_cols(cutadapt.meta.output %>%
                                         select(Sample_name, Locus),
                                       sample.names.df,
                                seqtab.nochim.hashes.df)
  seqtab.nochim.hashes.df %>%
    pivot_longer(cols = c(- Sample_name, -sample_id, - Locus),
                 names_to = "Hash",
                 values_to = "nReads") %>%
    filter(nReads > 0) -> current_asv
  write_csv(current_asv, ASV_file)    }else{
    #What do we do if you don't want hashes: two things - Change the header of the ASV table, write only one file
    seqtab.nochim.df %>%
      pivot_longer(cols = c(- Sample_name, - Locus),
                   names_to = "Sequence",
                   values_to = "nReads") %>%
      filter(nReads > 0) -> current_asv
    write_csv(current_asv, ASV_file)
  }
```
<br>

# QC: Track reads

Get the number of reads at each step. 

```{r include=FALSE}
getN <- function(x) sum(getUniques(x))
```

```{r}
qc.dat <- output.dada2 %>%
  select(-file1, -file2, -filtF1, -filtR1, -errF1, -errR1, -derepF1, -derepR1) %>%
  mutate_at(.vars = c("dadaF1", "dadaR1", "mergers"),
            ~ sapply(.x,getN)) %>%
  #  pull(outFs) -> test
  mutate(input = map_dbl(outFs, ~ .x[1]),
         filtered = map_dbl(outFs, ~ .x[2]),
         tabled  = rowSums(seqtab),
         nonchim = rowSums(seqtab.nochim)) %>%
  select(Sample_name,
         Locus,
         input,
         filtered,
         denoised_F = dadaF1,
         denoised_R = dadaR1,
         merged = mergers,
         tabled,
         nonchim)
write_csv(qc.dat, here(outdir,"dada2_qc_summary.csv"))

## drop
# if (keep.mid.files==FALSE){
#   unlink(here(filt.dir), recursive = T)
# }
```
<br>

Make output_summaryfig
```{r eval=FALSE}
qc.dat %>%
  mutate_if(is.numeric, as.integer) %>%
  pivot_longer(cols = c(-Sample_name, -Locus),
               names_to = "Step",
               values_to = "Number of Sequences") %>%
  mutate (Step = fct_relevel(Step,
                             levels = c( "input","filtered","denoised_F" ,"denoised_R" , "merged" , "tabled", "nonchim"))) %>%
  ggplot(aes(x = Step, y = `Number of Sequences`, group =  Sample_name, color = Sample_name)) +
  geom_line() +
  guides(color = "none")
```

It looks like the high proportion of bimeras that needed to be removed came from a few specific samples, so that's good. Based on the .csv file, it looks like those samples are: 

37-39: KAY 20 replicates (20-30% removed)

76-78: MARPT 02 replicates (5-7% removed)

58-60: CLAY 04 replicates (8-9% removed)

<br>


```{r eval=FALSE}
qc.dat %>%
  mutate_if(is.numeric, as.integer) %>%
  pivot_longer(cols = c(-Sample_name, -Locus),
               names_to = "Step",
               values_to = "Number of Sequences") %>%
  mutate (Step = fct_relevel(Step,
                             levels = c( "input","filtered","denoised_F" ,"denoised_R" , "merged" , "tabled", "nonchim"))) %>%
  mutate (group = ifelse(Sample_name %in% c(94,95,96), "Control", "Sample")) %>%
  ggplot(aes(x = Step, y = `Number of Sequences`/1000, color = group)) +
  geom_boxplot() +
  guides(color = "none") + theme_bw()
```
```{r}
png(here(outdir,'reads_per_step.png'))
qc.dat %>%
  mutate_if(is.numeric, as.integer) %>%
  pivot_longer(cols = c(-Sample_name, -Locus),
               names_to = "Step",
               values_to = "Number of Sequences") %>%
  mutate (Step = fct_relevel(Step,
                             levels = c( "input","filtered","denoised_F" ,"denoised_R" , "merged" , "tabled", "nonchim"))) %>%
  mutate (group = ifelse(Sample_name %in% c(94,95,96), "Control", "Sample")) %>%
  ggplot(aes(x = Step, y = `Number of Sequences`/1000, color = group)) +
  geom_boxplot() +
  guides(color = "none") + theme_bw()
dev.off()
```
