---
title: "DADA2"
subtitle: "last knit: `r format(Sys.time(), '%B %d, %Y')`"
author: "M Fisher (from Eily, Moncho)"
date: '2022-09-30'
output: 
  html_document:
    toc: yes
---


# Description

Run **DADA2** [tutorial here](https://benjjneb.github.io/dada2/tutorial.html) in order to get an amplicon sequence variant (ASV) table, which records the number of times each exact amplicon sequence variant was observed in each sample. 

Certain decisions have to be made throughout the script, so *do not just knit this script with the existing values*. Go through each code chunk in R first, then knit. Certain code chunks will not re-run when the script is knitted, to avoid over-writing existing files.

Input: demultiplexed fastq files, without barcodes / adapters / primers. 



<br>

# Set up

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if(!require("here")) {install.packages("here")}
if(!require("tidyverse")) {install.packages("tidyverse")}
if(!require("magrittr")) {install.packages("magrittr")}
if(!require("digest")) {install.packages("digest")}
if(!require("seqinr")) {install.packages("seqinr")}
# if(!requireNamespace("BiocManager", quietly = TRUE)){install.packages("BiocManager")}
# BiocManager::install("dada2", version = "3.10")
library(dada2)
library(digest)
library(seqinr)
```
<br>

User directories
```{r set up }
# root directory for cutadapt
cutadapt_dir <- "data/cutadapt"
# output directory
outdir <- "data/dada2"
```

User inputs
```{r}
run.num = 1

hash = TRUE  # rarely do you want hash = false (EA)

keep.mid.files = FALSE # I find I never look at these / use these and they just take up space (EA)
```
<br>
<br>


# Prep for DADA2

```{r message=FALSE, warning=FALSE}
run_cutadapt_dir = paste0(cutadapt_dir, "/noprimers")
```
<br>

read in sequencing metadata. set default trim length based on primer. 
```{r message=FALSE}
cutadapt.meta <- read_csv(here(run_cutadapt_dir, paste0("output.metadata.csv")))
marker        <- unique(cutadapt.meta$Locus)
print(marker)
```
```{r echo=FALSE}
if(marker=="Leray" | marker=="LerayXT"){
  trimming.length.r1 = 250
  trimming.length.r2 = 200
  message("trim lengths set as (r1,r1): ", trimming.length.r1, ",",trimming.length.r2)
} else if(marker=="BF3"){
  trimming.length.r1 = 260
  trimming.length.r2 = 200
} else{
  message("please manually enter trim length for this marker.")
}
```
<br>

read in file names.
```{r}
fnFs <- sort(list.files(path=here(run_cutadapt_dir), pattern="_R1_001.fastq.fastq", full.names = TRUE))
fnFs_simple <- str_remove(fnFs,pattern=paste0(here(run_cutadapt_dir),"/"))
fnRs <- sort(list.files(path=here(run_cutadapt_dir), pattern="_R2_001.fastq.fastq", full.names = TRUE))
```
<br>

get the sample names, which will be used to name the filtered files.
```{r}
# sample.names <- marker.meta$Sample_name
prefix <- paste0("Locus_",marker,"_")
suffix <- paste0("_L001_R1_001.fastq.fastq")
sample.names.df <- (cutadapt.meta %>% dplyr::select(file1) %>%
                      filter(file1 %in% fnFs_simple) %>%
                    mutate(sample_id=str_remove(file1,prefix)) %>%
                      mutate(sample_id=str_remove(sample_id,suffix)) %>%
                      separate(col=sample_id, into=c("sample_id","sample.num"), sep="_S")) %>% dplyr::select(sample_id)
sample.names <- as.character(sample.names.df$sample_id)
```
<br>

remove any files from the cutadapt metadata that don't have corresponding files in the post-cutadapt folder (the indexes that weren't used in the run)
```{r}
cutadapt.meta.output <- filter(cutadapt.meta, file1 %in% fnFs_simple)
```
<br>

write output directory path for filtered files in the run's cutadapt folder.
```{r}
filt.dir <- paste0(cutadapt_dir, "/noprimers_filtered")
```
<br>

create directories if they don't exist
```{r}
if(!dir.exists(here(filt.dir))){
  dir.create(path = here(filt.dir),recursive = T)
}
if(!dir.exists(here(outdir))){
  dir.create(path = here(outdir),recursive = T)
}
```
<br>

Double check the quality scores, **if not already completed in QC script 0**
Plot forward read quality scores, one sample per group
```{r eval=FALSE}
plotQualityProfile(fnFs[c(1,2,6,19,25)])
```
<br>

Plot forward read quality scores, one sample per group
```{r eval=FALSE}
plotQualityProfile(fnRs[c(1,2,6,19,25)])
```
<br>


manually enter trim lengths - if there isn't a default for the marker, or if the defaults are too long based on the quality scores (see script 0_qc)
```{r eval=FALSE}
trimming.length.r1 = 225
trimming.length.r2 = 200
```
<br>
<br>

# DADA2

## Filter and trim

- `truncLen` truncates the sequence length, and should be based on per-base quality scores. I'm using the length that Eily suggested , 120bp.
- `maxEE` discards reads with high "expected errors" = $\sum(10^{-Q/10})$
- `rm.phix` removes reads from PhiX spike
- use `compress` to gzip the output fastqs

```{r}
output.dada2 <- cutadapt.meta.output %>%
  #filter(rc == 1) %>% # ONLY SELECT THE BACKWARDS ONES (1) OR FORWARDS ONES (0)
  mutate(basename=sample.names) %>%
  mutate(file1  = here(run_cutadapt_dir, file1),
         file2  = here(run_cutadapt_dir, file2),
         filtF1 = here(filt.dir, paste0(basename, "_F1_filt.fastq.gz")),
         filtR1 = here(filt.dir, paste0(basename, "_R1_filt.fastq.gz"))) %>%
  select(-basename) %>% 
  mutate (outFs = pmap(.l= list (file1, filtF1, file2, filtR1),
                       .f = function(a, b, c, d) {
                         filterAndTrim(a,b,c,d,
                                       truncLen=c(trimming.length.r1,trimming.length.r2),
                                       maxN=0, maxEE=c(2,2), truncQ=2, rm.phix=TRUE,
                                       compress=TRUE, multithread=FALSE )
                       } ),
          errF1 = map(filtF1, ~ learnErrors(.x, multithread=FALSE,verbose = 0)),     # Calculate errors
          errR1 = map(filtR1, ~ learnErrors(.x, multithread=FALSE,verbose = 0)),
          derepF1 = map(filtF1, derepFastq),                   # dereplicate seqs
          derepR1 = map(filtR1, derepFastq),
          dadaF1  = map2(derepF1,errF1, ~ dada(.x, err = .y, multithread = FALSE)),  # dada2
          dadaR1  = map2(derepR1,errR1, ~ dada(.x, err = .y, multithread = FALSE)),
          mergers = pmap(.l = list(dadaF1,derepF1, dadaR1,derepR1),                 # merge things
                         .f = mergePairs ))

if (keep.mid.files==TRUE){
  write_rds(output.dada2, path = here(outdir, "output.halfway.rds"))}
```
Sample 1 - 176269 reads in 13513 unique sequences.
Sample 1 - 186720 reads in 12866 unique sequences.
Sample 1 - 220428 reads in 14532 unique sequences.
Sample 1 - 137377 reads in 19566 unique sequences.
Sample 1 - 172368 reads in 22327 unique sequences.
Sample 1 - 186817 reads in 24612 unique sequences.
Sample 1 - 52141 reads in 5011 unique sequences.
Sample 1 - 72836 reads in 6509 unique sequences.
Sample 1 - 160955 reads in 11378 unique sequences.
Sample 1 - 232423 reads in 13558 unique sequences.
Sample 1 - 109892 reads in 9890 unique sequences.
Sample 1 - 123853 reads in 11515 unique sequences.
Sample 1 - 28525 reads in 4361 unique sequences.
Sample 1 - 18739 reads in 3094 unique sequences.
Sample 1 - 5844 reads in 1229 unique sequences.
Sample 1 - 217270 reads in 27800 unique sequences.
Sample 1 - 251522 reads in 31134 unique sequences.
Sample 1 - 225438 reads in 26201 unique sequences.
Sample 1 - 184215 reads in 22235 unique sequences.
Sample 1 - 139183 reads in 20789 unique sequences.
Sample 1 - 176752 reads in 22017 unique sequences.
Sample 1 - 91214 reads in 10722 unique sequences.
Sample 1 - 184241 reads in 19666 unique sequences.
Sample 1 - 164785 reads in 17443 unique sequences.
Sample 1 - 261581 reads in 30094 unique sequences.
Sample 1 - 148850 reads in 17239 unique sequences.
Sample 1 - 323767 reads in 34021 unique sequences.
Sample 1 - 212000 reads in 20832 unique sequences.
Sample 1 - 200597 reads in 20600 unique sequences.
Sample 1 - 181109 reads in 17517 unique sequences.
Sample 1 - 167261 reads in 24169 unique sequences.
Sample 1 - 186867 reads in 18221 unique sequences.
Sample 1 - 203053 reads in 19160 unique sequences.
Sample 1 - 218980 reads in 23434 unique sequences.
Sample 1 - 261839 reads in 28307 unique sequences.
Sample 1 - 223980 reads in 21814 unique sequences.
Sample 1 - 172974 reads in 29632 unique sequences.
Sample 1 - 164833 reads in 27230 unique sequences.
Sample 1 - 199381 reads in 28204 unique sequences.
Sample 1 - 245653 reads in 20160 unique sequences.
Sample 1 - 223353 reads in 18923 unique sequences.
Sample 1 - 224495 reads in 19734 unique sequences.
Sample 1 - 154511 reads in 21228 unique sequences.
Sample 1 - 154331 reads in 23753 unique sequences.
Sample 1 - 147926 reads in 18360 unique sequences.
Sample 1 - 141334 reads in 10659 unique sequences.
Sample 1 - 134118 reads in 11291 unique sequences.
Sample 1 - 178967 reads in 12658 unique sequences.
Sample 1 - 119612 reads in 9865 unique sequences.
Sample 1 - 72720 reads in 7078 unique sequences.
Sample 1 - 37327 reads in 4103 unique sequences.
Sample 1 - 270588 reads in 17163 unique sequences.
Sample 1 - 241365 reads in 17364 unique sequences.
Sample 1 - 269831 reads in 17449 unique sequences.
Sample 1 - 223688 reads in 11635 unique sequences.
Sample 1 - 184081 reads in 10256 unique sequences.
Sample 1 - 191891 reads in 10073 unique sequences.
Sample 1 - 76183 reads in 7680 unique sequences.
Sample 1 - 197533 reads in 16908 unique sequences.
Sample 1 - 233064 reads in 19058 unique sequences.
Sample 1 - 55186 reads in 4629 unique sequences.
Sample 1 - 187420 reads in 12348 unique sequences.
Sample 1 - 147230 reads in 14758 unique sequences.
Sample 1 - 314207 reads in 24643 unique sequences.
Sample 1 - 220035 reads in 19584 unique sequences.
Sample 1 - 268527 reads in 22525 unique sequences.
Sample 1 - 101283 reads in 6978 unique sequences.
Sample 1 - 17382 reads in 2387 unique sequences.
Sample 1 - 30562 reads in 2339 unique sequences.
Sample 1 - 79506 reads in 5065 unique sequences.
Sample 1 - 167954 reads in 9594 unique sequences.
Sample 1 - 157712 reads in 8320 unique sequences.
Sample 1 - 87170 reads in 5718 unique sequences.
Sample 1 - 226828 reads in 11239 unique sequences.
Sample 1 - 148936 reads in 7110 unique sequences.
Sample 1 - 134817 reads in 14673 unique sequences.
Sample 1 - 140179 reads in 14792 unique sequences.
Sample 1 - 202916 reads in 13341 unique sequences.
Sample 1 - 29125 reads in 2080 unique sequences.
Sample 1 - 53227 reads in 3035 unique sequences.
Sample 1 - 68574 reads in 3658 unique sequences.
Sample 1 - 369556 reads in 29830 unique sequences.
Sample 1 - 393294 reads in 31310 unique sequences.
Sample 1 - 427837 reads in 35807 unique sequences.
Sample 1 - 6495 reads in 837 unique sequences.
Sample 1 - 74022 reads in 7729 unique sequences.
Sample 1 - 53119 reads in 4347 unique sequences.
Sample 1 - 200216 reads in 11753 unique sequences.
Sample 1 - 114973 reads in 12479 unique sequences.
Sample 1 - 132229 reads in 8624 unique sequences.
Sample 1 - 4069 reads in 720 unique sequences.
Sample 1 - 29 reads in 16 unique sequences.
Sample 1 - 176269 reads in 18052 unique sequences.
Sample 1 - 186720 reads in 17532 unique sequences.
Sample 1 - 220428 reads in 16484 unique sequences.
Sample 1 - 137377 reads in 28329 unique sequences.
Sample 1 - 172368 reads in 25701 unique sequences.
Sample 1 - 186817 reads in 27882 unique sequences.
Sample 1 - 52141 reads in 5626 unique sequences.
Sample 1 - 72836 reads in 8420 unique sequences.
Sample 1 - 160955 reads in 16810 unique sequences.
Sample 1 - 232423 reads in 21598 unique sequences.
Sample 1 - 109892 reads in 10267 unique sequences.
Sample 1 - 123853 reads in 13065 unique sequences.
Sample 1 - 28525 reads in 3717 unique sequences.
Sample 1 - 18739 reads in 3519 unique sequences.
Sample 1 - 5844 reads in 1125 unique sequences.
Sample 1 - 217270 reads in 32127 unique sequences.
Sample 1 - 251522 reads in 30163 unique sequences.
Sample 1 - 225438 reads in 29203 unique sequences.
Sample 1 - 184215 reads in 23548 unique sequences.
Sample 1 - 139183 reads in 19616 unique sequences.
Sample 1 - 176752 reads in 20463 unique sequences.
Sample 1 - 91214 reads in 11080 unique sequences.
Sample 1 - 184241 reads in 21213 unique sequences.
Sample 1 - 164785 reads in 18864 unique sequences.
Sample 1 - 261581 reads in 33668 unique sequences.
Sample 1 - 148850 reads in 24612 unique sequences.
Sample 1 - 323767 reads in 37269 unique sequences.
Sample 1 - 212000 reads in 23885 unique sequences.
Sample 1 - 200597 reads in 21566 unique sequences.
Sample 1 - 181109 reads in 20935 unique sequences.
Sample 1 - 167261 reads in 21953 unique sequences.
Sample 1 - 186867 reads in 22591 unique sequences.
Sample 1 - 203053 reads in 25092 unique sequences.
Sample 1 - 218980 reads in 29934 unique sequences.
Sample 1 - 261839 reads in 31140 unique sequences.
Sample 1 - 223980 reads in 24535 unique sequences.
Sample 1 - 172974 reads in 27031 unique sequences.
Sample 1 - 164833 reads in 25906 unique sequences.
Sample 1 - 199381 reads in 32270 unique sequences.
Sample 1 - 245653 reads in 23374 unique sequences.
Sample 1 - 223353 reads in 23982 unique sequences.
Sample 1 - 224495 reads in 22622 unique sequences.
Sample 1 - 154511 reads in 18345 unique sequences.
Sample 1 - 154331 reads in 16922 unique sequences.
Sample 1 - 147926 reads in 19913 unique sequences.
Sample 1 - 141334 reads in 12993 unique sequences.
Sample 1 - 134118 reads in 13934 unique sequences.
Sample 1 - 178967 reads in 15210 unique sequences.
Sample 1 - 119612 reads in 13197 unique sequences.
Sample 1 - 72720 reads in 8052 unique sequences.
Sample 1 - 37327 reads in 4329 unique sequences.
Sample 1 - 270588 reads in 22461 unique sequences.
Sample 1 - 241365 reads in 18675 unique sequences.
Sample 1 - 269831 reads in 19102 unique sequences.
Sample 1 - 223688 reads in 15156 unique sequences.
Sample 1 - 184081 reads in 17000 unique sequences.
Sample 1 - 191891 reads in 13637 unique sequences.
Sample 1 - 76183 reads in 8711 unique sequences.
Sample 1 - 197533 reads in 19181 unique sequences.
Sample 1 - 233064 reads in 23498 unique sequences.
Sample 1 - 55186 reads in 6906 unique sequences.
Sample 1 - 187420 reads in 16393 unique sequences.
Sample 1 - 147230 reads in 10773 unique sequences.
Sample 1 - 314207 reads in 35830 unique sequences.
Sample 1 - 220035 reads in 26299 unique sequences.
Sample 1 - 268527 reads in 30409 unique sequences.
Sample 1 - 101283 reads in 6539 unique sequences.
Sample 1 - 17382 reads in 1505 unique sequences.
Sample 1 - 30562 reads in 2987 unique sequences.
Sample 1 - 79506 reads in 6403 unique sequences.
Sample 1 - 167954 reads in 12805 unique sequences.
Sample 1 - 157712 reads in 14568 unique sequences.
Sample 1 - 87170 reads in 6680 unique sequences.
Sample 1 - 226828 reads in 13315 unique sequences.
Sample 1 - 148936 reads in 12203 unique sequences.
Sample 1 - 134817 reads in 10908 unique sequences.
Sample 1 - 140179 reads in 10269 unique sequences.
Sample 1 - 202916 reads in 17307 unique sequences.
Sample 1 - 29125 reads in 3300 unique sequences.
Sample 1 - 53227 reads in 5917 unique sequences.
Sample 1 - 68574 reads in 4954 unique sequences.
Sample 1 - 369556 reads in 41499 unique sequences.
Sample 1 - 393294 reads in 40545 unique sequences.
Sample 1 - 427837 reads in 54199 unique sequences.
Sample 1 - 6495 reads in 1065 unique sequences.
Sample 1 - 74022 reads in 5699 unique sequences.
Sample 1 - 53119 reads in 5402 unique sequences.
Sample 1 - 200216 reads in 26185 unique sequences.
Sample 1 - 114973 reads in 7279 unique sequences.
Sample 1 - 132229 reads in 10206 unique sequences.
Sample 1 - 4069 reads in 1430 unique sequences.
Sample 1 - 29 reads in 13 unique sequences.



<br>

The sequence table is a matrix with rows corresponding to (and named by) the samples, and columns corresponding to (and named by) the sequence variants. 
```{r}
seqtab <- makeSequenceTable(output.dada2$mergers)
dim(seqtab)
```
92 | 2749
<br>

Inspect distribution of sequence lengths:
```{r echo=FALSE}
table(nchar(getSequences(seqtab)))

table(nchar(getSequences(seqtab))) %>% as.data.frame() %>%
  mutate(Length=as.character(Var1),
         Length=as.numeric(Length)) %>%
  ggplot( aes(x=Length,y=Freq)) +
  geom_col() + theme_bw()
```

65% are 313 (yay!) with some larger (quite a few at 352) and some shorter (about 100 at 304)

```{r}
write.csv(table(nchar(getSequences(seqtab))) %>% as.data.frame() %>%
  mutate(Length=as.character(Var1),
         Length=as.numeric(Length)), file=here(outdir,'dada2_filtered_seqLengths.csv'))
```
<br>


## Remove chimeras

```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, method="consensus", verbose=TRUE)
dim(seqtab.nochim)

seqtab.nochim.df <- as.data.frame(seqtab.nochim)
```
Identified 1067 bimeras out of 2749 input sequences.
[1]   92 | 1682

weird, that's a higher proportion of bimeras than I'm used to seeing. 
<br>

## Write output

Copy the metadata so it is all in one place
```{r}
cutadapt.meta.output %>% write_csv(here(outdir,"dada2.metadata.csv"))
```
<br>

Output file names
```{r}
conv_file <- here(outdir,"hash_key.csv")
conv_file.fasta <- here(outdir,"hash_key.fasta")
ASV_file <-  here(outdir,"ASV_table.csv")
```
<br>

If using hashes, set up the output table with hash IDs and write it out.
```{r}
if (hash==TRUE)
{conv_table <- tibble( Hash = "", Sequence ="")
  map_chr (colnames(seqtab.nochim.df), ~ digest(.x, algo = "sha1", serialize = F, skip = "auto")) -> Hashes
  conv_table <- tibble (Hash = Hashes,
                        Sequence = colnames(seqtab.nochim.df))
  seqtab.nochim.hashes.df <- seqtab.nochim.df
  colnames(seqtab.nochim.hashes.df) <- Hashes

  write_csv(conv_table, conv_file) # write the table into a file
  write.fasta(sequences = as.list(conv_table$Sequence),
              names     = as.list(conv_table$Hash),
              file.out = conv_file.fasta)
  seqtab.nochim.hashes.df <- bind_cols(cutadapt.meta.output %>%
                                         select(Sample_name, Locus),
                                       sample.names.df,
                                seqtab.nochim.hashes.df)
  seqtab.nochim.hashes.df %>%
    pivot_longer(cols = c(- Sample_name, -sample_id, - Locus),
                 names_to = "Hash",
                 values_to = "nReads") %>%
    filter(nReads > 0) -> current_asv
  write_csv(current_asv, ASV_file)    }else{
    #What do we do if you don't want hashes: two things - Change the header of the ASV table, write only one file
    seqtab.nochim.df %>%
      pivot_longer(cols = c(- Sample_name, - Locus),
                   names_to = "Sequence",
                   values_to = "nReads") %>%
      filter(nReads > 0) -> current_asv
    write_csv(current_asv, ASV_file)
  }
```
<br>

# QC: Track reads

Get the number of reads at each step. 

```{r include=FALSE}
getN <- function(x) sum(getUniques(x))
```

```{r}
qc.dat <- output.dada2 %>%
  select(-file1, -file2, -filtF1, -filtR1, -errF1, -errR1, -derepF1, -derepR1) %>%
  mutate_at(.vars = c("dadaF1", "dadaR1", "mergers"),
            ~ sapply(.x,getN)) %>%
  #  pull(outFs) -> test
  mutate(input = map_dbl(outFs, ~ .x[1]),
         filtered = map_dbl(outFs, ~ .x[2]),
         tabled  = rowSums(seqtab),
         nonchim = rowSums(seqtab.nochim)) %>%
  select(Sample_name,
         Locus,
         input,
         filtered,
         denoised_F = dadaF1,
         denoised_R = dadaR1,
         merged = mergers,
         tabled,
         nonchim)
write_csv(qc.dat, here(outdir,"dada2_qc_summary.csv"))

## drop
# if (keep.mid.files==FALSE){
#   unlink(here(filt.dir), recursive = T)
# }
```
<br>

Make output_summaryfig
```{r eval=FALSE}
qc.dat %>%
  mutate_if(is.numeric, as.integer) %>%
  pivot_longer(cols = c(-Sample_name, -Locus),
               names_to = "Step",
               values_to = "Number of Sequences") %>%
  mutate (Step = fct_relevel(Step,
                             levels = c( "input","filtered","denoised_F" ,"denoised_R" , "merged" , "tabled", "nonchim"))) %>%
  ggplot(aes(x = Step, y = `Number of Sequences`, group =  Sample_name, color = Sample_name)) +
  geom_line() +
  guides(color = "none")
```

It looks like the high proportion of bimeras that needed to be removed came from a few specific samples, so that's good. Based on the .csv file, it looks like those samples are: 

37-39: KAY 20 replicates (20-30% removed)

76-78: MARPT 02 replicates (5-7% removed)

58-60: CLAY 04 replicates (8-9% removed)

<br>


```{r eval=FALSE}
qc.dat %>%
  mutate_if(is.numeric, as.integer) %>%
  pivot_longer(cols = c(-Sample_name, -Locus),
               names_to = "Step",
               values_to = "Number of Sequences") %>%
  mutate (Step = fct_relevel(Step,
                             levels = c( "input","filtered","denoised_F" ,"denoised_R" , "merged" , "tabled", "nonchim"))) %>%
  mutate (group = ifelse(Sample_name %in% c(94,95,96), "Control", "Sample")) %>%
  ggplot(aes(x = Step, y = `Number of Sequences`/1000, color = group)) +
  geom_boxplot() +
  guides(color = "none") + theme_bw()
```
```{r}
png(here(outdir,'reads_per_step.png'))
qc.dat %>%
  mutate_if(is.numeric, as.integer) %>%
  pivot_longer(cols = c(-Sample_name, -Locus),
               names_to = "Step",
               values_to = "Number of Sequences") %>%
  mutate (Step = fct_relevel(Step,
                             levels = c( "input","filtered","denoised_F" ,"denoised_R" , "merged" , "tabled", "nonchim"))) %>%
  mutate (group = ifelse(Sample_name %in% c(94,95,96), "Control", "Sample")) %>%
  ggplot(aes(x = Step, y = `Number of Sequences`/1000, color = group)) +
  geom_boxplot() +
  guides(color = "none") + theme_bw()
dev.off()
```
